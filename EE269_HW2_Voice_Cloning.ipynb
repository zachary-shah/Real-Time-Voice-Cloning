{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachary-shah/Real-Time-Voice-Cloning/blob/ee269-voice-cloning/EE269_HW2_Voice_Cloning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nqvJn5sXouZ"
      },
      "source": [
        "# EE269 HW2: Voice Cloning with neural models\n",
        "\n",
        "In this homework question, we will explore Voice Cloning using state-of-the-art neural modeling approaches. This assignment is based on a voice cloning framework developed by Corentin Jemine described [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning). Roughly, there is a neural network which generates a speaker embedding from audio, and a neural Text-to-Speech (TTS) system which conditions on this speaker embedding to create an adjusted audio sample for a given text input.\n",
        "\n",
        "The goals for this assignment are:\n",
        "* Introduce the Voice Cloning system and interface with the pre-trained models\n",
        "* Visualize mel spectograms and MFCCs from real vs synthesized audio examples to compare them\n",
        "* Explore how noise filtering affects the quality of model outputs\n",
        "* Clone your own voice :)\n",
        "\n",
        "**Note:** You will need to make a copy of this Colab notebook in your Google Drive before you can edit it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "133ELZsy1qAf"
      },
      "source": [
        "# Setup\n",
        "\n",
        "First, make sure you have a GPU enabled in Colab by selecting Runtime -> Change Runtime Type -> T4 GPU.\n",
        "\n",
        "We made a branch of the original framework for this homework assignment. The two blocks below set this up and install all required packages. All necessary files for this assignment should be contained within the `Real-Time-Voice-Cloning` folder once the github branch has been cloned.\n",
        "\n",
        "*Note*: When installing the required packages, may get an error that ```pip's dependency resolver does not currently take into account all the packages that are installed.``` This is most likely fine, as long as the code cells in subsequent sections are able to run. Otherwise, try to install the packages in ```requirements.txt``` independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6sCauYLe4jF"
      },
      "outputs": [],
      "source": [
        "!git clone --branch ee269-voice-cloning https://github.com/zachary-shah/Real-Time-Voice-Cloning.git\n",
        "%cd Real-Time-Voice-Cloning\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afhCnkNtqFnO"
      },
      "outputs": [],
      "source": [
        "!apt install -q libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9oQxxGz-5Lz"
      },
      "source": [
        "**For convenience, here is a helper function for plotting mel spectrograms:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_w8fOs9goPt"
      },
      "outputs": [],
      "source": [
        "# packages for problem\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "# helper function to plot a mel spectrogram\n",
        "# arguments: (wave array, sampling rate, number of mel bins, max frequency of mel scale)\n",
        "def plot_melspectrogram(wav, sr, n_mels=256, fmax=4096, title=None,\n",
        "                        fig=None, ax=None, show_legend=True):\n",
        "\n",
        "    if ax == None:\n",
        "        fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
        "    M = librosa.feature.melspectrogram(y=wav, sr=sr, n_mels=n_mels, fmax=fmax, n_fft=2048)\n",
        "    M_db = librosa.power_to_db(M, ref=np.max)\n",
        "    img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax, fmax=fmax)\n",
        "    if show_legend:\n",
        "        ax.set(title='Mel spectrogram display')\n",
        "        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
        "    if title is not None:\n",
        "        ax.set(title=title)\n",
        "\n",
        "# seed for repeatability\n",
        "seed = 269269269269"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcp7vAfBwmtu"
      },
      "source": [
        "# Part 1: Learn how to voice clone. (5 pts)\n",
        "\n",
        "We will first experiment with an example voice from the dataset used in the development of the pretrained vocoder and synthesizer models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Loading Data (1 point)\n",
        "\n",
        "First, load in the audio file at ```audio_path``` below and listen to the audio file (you may find ```Ipython.display.Audio``` useful for this). Use the helper function above to plot the mel spectrogram.\n"
      ],
      "metadata": {
        "id": "00SOmqwnSkwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-lVjqA4wi6J"
      },
      "outputs": [],
      "source": [
        "## TASK 1: Load the audio file and plot it's mel spectrogram using the helper function provided\n",
        "\n",
        "# example audio file\n",
        "audio_path = \"data/dataset_example.wav\"\n",
        "\n",
        "#########################\n",
        "###  YOUR CODE HERE   ###\n",
        "#########################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEaM7RWzADyy"
      },
      "source": [
        "### Cloning a Voice (4 points)\n",
        "\n",
        "Now, we will use the voice cloning models to transfer the style of the above voice into new text (also known as an utterance).\n",
        "\n",
        "For simplicity, we have abstracted inference of the voice cloning models into a ```VoiceCloner()``` object, which will load the pre-trained encoder, synthesizer, and vocoder models upon initialization. It has two functions useful for generating cloned audio:\n",
        "\n",
        "```VoiceCloner.embed_style()```: Generates the voice style embeddings\n",
        "> Parameters:\n",
        ">> ```in_fpath```: Path to audio file containing an utterance\n",
        ">\n",
        ">  Returns:\n",
        ">> ```emb```: embedding vector for utterance (numpy.ndarray)\n",
        "\n",
        "```VoiceCloner.voice_clone()```: Generates an utterance cloned in a style described by embeddings\n",
        "> Parameters:\n",
        ">> ```text```: Text for utterance to transfer style onto (str)\n",
        ">\n",
        ">> ```emb```: Voice embedding vector (numpy.ndarray)\n",
        ">\n",
        ">> ```out_fpath```: filepath to write cloned utterance to (str)\n",
        ">\n",
        "> Returns:\n",
        ">> ```generated_wav```: array representation of cloned utterance (numpy.ndarray)\n",
        "\n",
        "\n",
        "With this information, clone the voice in the file ```data/dataset_example.wav``` which was plotted above.\n",
        "\n",
        "After cloning, listen to the cloned utterance and plot it's mel spectrogram. Verbally compare the quality of the clone to the signals for the true voice.\n",
        "\n",
        "Feel free to try different prompts, but for the final submission, please use the default utterance for Parts 1 and 2: \"Hello. I am a voice clone. Nice to meet you!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnqlCZxNwiEn"
      },
      "outputs": [],
      "source": [
        "# Set up the voice cloner\n",
        "from voicecloner.sampler import VoiceCloner\n",
        "cloner = VoiceCloner(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zw2jwOPCm7T"
      },
      "outputs": [],
      "source": [
        "# TASK: Clone the dataset example into the below utterance using the cloner object.\n",
        "\n",
        "text = \"Hello. I am a voice clone. Nice to meet you!\"\n",
        "\n",
        "#########################\n",
        "###  YOUR CODE HERE   ###\n",
        "#########################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzaqwh7D0py9"
      },
      "outputs": [],
      "source": [
        "# TASK: plot cloned spectrogram and listen to audio\n",
        "\n",
        "#########################\n",
        "###  YOUR CODE HERE   ###\n",
        "#########################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqG9DTCF01T1"
      },
      "source": [
        "# Part 2: Cloning with Noisy Signals (10 pts)\n",
        "\n",
        "In the section before, the audio file we generated the style embeddings from was considerably clean.\n",
        "\n",
        "In this section, we will examine what happens when we try to clone with a noisy recording. During training, the model will see mostly clean audio as it learns the style embeddings, so it is interesting to examine what the model does during inference on a noisy signal.\n",
        "\n",
        "We will test this out with a recording of Dr. Pilanci from a previous lecture. Unfortunately, this recording was corrupted by Additive White Gaussian Noise. Can we still clone his voice?\n",
        "\n",
        "TASK: Following the procedure in Part 1, read in the audio file ```data/dr_pilanci_lecture_clip.wav```, listen to the audio and plot the mel spectrogram.\n",
        "\n",
        "Write out any visual differences you see between the spectrogram for a noisy audio clip in comparison to the mel spectrograms in the training distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NXMUOSf0z9w"
      },
      "outputs": [],
      "source": [
        "# TASK: Listen to and plot the mel spectrogram of Dr. Pilanci's noisy lecture clip.\n",
        "\n",
        "noisy_lecture_path = \"data/dr_pilanci_lecture_clip.wav\"\n",
        "\n",
        "#################\n",
        "### CODE HERE ###\n",
        "#################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzreU14F16lm"
      },
      "source": [
        "> TASK: Write Observations here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LltKsppY2dMl"
      },
      "source": [
        "## Voice Cloning on raw noisy signal (2 points)\n",
        "\n",
        "Following the process of part 1, clone Dr. Pilanci's voice saying the text \"Hello. I am a voice clone. Nice to meet you!\"\n",
        "\n",
        "Listen to the cloned audio and plot it's spectrogram. How does the cloning quality compare to Part 1?\n",
        "\n",
        "*Note: You should not need to create a new VoiceCloner() object.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol9ZHedM1F3V"
      },
      "outputs": [],
      "source": [
        "# TASK: Clone utterance with noisy lecture audio, and plot mel spectrogram.\n",
        "\n",
        "#################\n",
        "### CODE HERE ###\n",
        "#################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uggu9viLH9oh"
      },
      "source": [
        "> TASK: Describe quality here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RVUB0993ITF"
      },
      "source": [
        "## Denoising (5 points)\n",
        "\n",
        "Now, we would like to see if we can improve the model output quality by denoising the input signal.\n",
        "\n",
        "To eliminate signal noise, we will employ a method known as \"spectral gating\". This algorithm functions by generating a spectrogram of a signal and determining a noise threshold (or gate) for each frequency band within that signal/noise. This threshold then aids in the creation of a mask that filters out noise beneath the varying frequency threshold. This has been implemented in the Python package [noisereduce](https://pypi.org/project/noisereduce/), which we have already installed in the setup.\n",
        "\n",
        "In the code below, denoise the clip of Dr. Pilanci's lecture using spectral gating. This can be done with ```noisereduce.reduce_noise()```. You may need to play with some of the noise thresholding parameters (such as prop_decrease, time_constant_s, n_std_thresh_stationary, sigmoid_slope_nonstationary) to minimize signal loss and maintain a well-balanced signal-to-noise ratio in the denoised audio.\n",
        "\n",
        "Plot the spectrogram of the original vs. denoised audio. The goal is to get the denoised spectrogram to look like a sample from the same distribution of audio that the model was trained on (we saw an example of this in Part 1).\n",
        "\n",
        "Make sure to listen to the denoised audio as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQDEflNv1GQh"
      },
      "outputs": [],
      "source": [
        "# TASK: Filter noisy lecture clip look more similar to the example in the training dataset.\n",
        "# Plot the mel spectrogram for the noisy and denoised clips side by side.\n",
        "\n",
        "import noisereduce\n",
        "\n",
        "#################\n",
        "### CODE HERE ###\n",
        "#################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxpCiAiz4ntg"
      },
      "source": [
        "## Voice Cloning on cleaned signal (3 points)\n",
        "\n",
        "With the signal you just cleaned, generate a voice clone of the same text: \"Hello. I am a voice clone. Nice to meet you!\"\n",
        "\n",
        "Listen to the cloned audio and plot it's spectrogram. How does the cloning quality compare to that of the clone from the noisy audio?\n",
        "\n",
        "*Note: If the clone sounds worse than before, you may need to adjust the denoising parameters.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekTXt2r13H4T"
      },
      "outputs": [],
      "source": [
        "# TASK: Clone utterance with the cleaned audio clip.\n",
        "# Plot the mel spectrogram of the clone.\n",
        "\n",
        "#################\n",
        "### CODE HERE ###\n",
        "#################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> TASK: Describe quality of clone"
      ],
      "metadata": {
        "id": "8r94t4rsWaL3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulx8NWQVYioK"
      },
      "source": [
        "# Part 3: Clone **your** voice (5 points)\n",
        "\n",
        "Now that you know how to voice clone, why not try it on yourself?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXxChMt7ndpK"
      },
      "source": [
        "### Record your voice.\n",
        "\n",
        "Below we have provided a function that will record audio from your computer microphone directly through colab. Make sure to enable microphone access to your browser before running.\n",
        "\n",
        "Record a 5-second clip speaking any text you would like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmsP6vzzhACD"
      },
      "outputs": [],
      "source": [
        "from voicecloner.interface import record\n",
        "\n",
        "audio_path = record(sec=5, out_fpath=\"data/my_voice.wav\")\n",
        "ipd.Audio(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeKbpfUpLztO"
      },
      "source": [
        "## Clone your voice (2 points)\n",
        "\n",
        "As we have done before, clone your own voice. This time, use the voice cloner to generate the exact same text you recorded.\n",
        "\n",
        "If the recording quality is low, try enhancing the audio SNR using ```noisereduce``` or any other method you think might help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDIm-LC573_R"
      },
      "outputs": [],
      "source": [
        "# TASK: Clone your voice saying the text your recorded in the block above.\n",
        "\n",
        "###################\n",
        "#### CODE HERE ####\n",
        "###################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EfQHbJ_r9M7"
      },
      "source": [
        "### Plot Spectrograms (1 point)\n",
        "\n",
        "Plot spectrograms for your original speech and synthesized speech next to one another. Describe the differences you notice when listening in the audio, and how you think such differences register in the spectrogram plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO8GeUQv5KcW"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "#### CODE HERE ####\n",
        "###################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> TASK: Write observations here"
      ],
      "metadata": {
        "id": "KQjBy368T3pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot MFCCs (2 points)\n",
        "\n",
        "Thus far, we have been representing audio in its mel spectrogram. We can also represent an audio signal through its MFCCs (Mel-Frequency Cepstral Coefficients), which capture the short-term power spectrum of sound and are especially sensitive to the human ear's perception of speech. By modeling the unique characteristics of an individual's voice, MFCCs can be used to effectively differentiate and classify different speakers.\n",
        "\n",
        "Plot the first 20 MFCC coefficients for both your real and cloned utterances side by side. You can use [librosa.feature.mfcc](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html) for this, which will compute MFCC coefficients across different frames.\n",
        "\n",
        "For each instance, plot both the average MFCC coefficients (compute this average over all frames), as well as the log-normalized MFCC coefficients plotted for all frames as an image (an example is shown [here](https://haythamfayek.com/assets/posts/post1/mfcc_raw.jpg))."
      ],
      "metadata": {
        "id": "mrnwzn1C4L8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK: Compute MFCCs of real and cloned voice and plot MFCC matrix as an image\n",
        "\n",
        "###################\n",
        "#### CODE HERE ####\n",
        "###################\n"
      ],
      "metadata": {
        "id": "bLT5MAJ14K-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK: Plot the average MFCCs across the frame dimension for real and cloned voices\n",
        "\n",
        "###################\n",
        "#### CODE HERE ####\n",
        "###################\n"
      ],
      "metadata": {
        "id": "I7TOqZfk3Ut1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}